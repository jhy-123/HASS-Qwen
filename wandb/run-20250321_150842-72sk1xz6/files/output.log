You are using a model of type qwen2 to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
  0%|‚ñè                                                                                                                                                   | 11/9500 [00:16<3:51:04,  1.46s/it]
Traceback (most recent call last):
  File "/root/miniconda3/envs/eagle/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/miniconda3/envs/eagle/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jhy/jianghaoyun/sps/HASS_Qwen/train/main_hass.py", line 401, in <module>
    predict = model(hidden_states, input_ids, attention_mask, q_hidden_states=q_hidden_states)
  File "/root/miniconda3/envs/eagle/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/eagle/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/eagle/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/root/miniconda3/envs/eagle/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/eagle/lib/python3.10/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
  File "/root/miniconda3/envs/eagle/lib/python3.10/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/root/miniconda3/envs/eagle/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 14, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/jhy/jianghaoyun/sps/HASS_Qwen/model/cnets_hass.py", line 659, in forward
    layer_outputs = torch.utils.checkpoint.checkpoint(
  File "/root/miniconda3/envs/eagle/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 251, in checkpoint
    return _checkpoint_without_reentrant(
  File "/root/miniconda3/envs/eagle/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 432, in _checkpoint_without_reentrant
    output = function(*args, **kwargs)
  File "/home/jhy/jianghaoyun/sps/HASS_Qwen/model/cnets_hass.py", line 655, in custom_forward
    return module(*inputs, past_key_value, output_attentions)
  File "/root/miniconda3/envs/eagle/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jhy/jianghaoyun/sps/HASS_Qwen/model/cnets_hass.py", line 457, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/envs/eagle/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jhy/jianghaoyun/sps/HASS_Qwen/model/cnets_hass.py", line 329, in forward
    align_big_mask = align_big_mask.view(1, 1, 1, q_len, q_len).to(attn_weights.device)
KeyboardInterrupt
